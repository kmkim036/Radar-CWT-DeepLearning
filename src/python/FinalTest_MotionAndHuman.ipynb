{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "MotionAndHuman_FinalTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# setting for load google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "b0gSl2pikPfu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Input, Flatten, Add, BatchNormalization, Activation, Input\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "outputs": [],
      "metadata": {
        "id": "pC3LkIDAkZcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "convlayers = 4\n",
        "denseparam = 128"
      ],
      "outputs": [],
      "metadata": {
        "id": "SS0aMtE1l2MZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "augment_ratio = 9\n",
        "classnum = 3     # class 개수\n",
        "try_num = 10   # 같은 조건에서 몇번 반복할지\n",
        "\n",
        "date = '220132'\n",
        "count = 400\n",
        "\n",
        "lr = 0.001\n",
        "bs = 64\n",
        "wsr = 0.15\n",
        "\n",
        "test_label = np.zeros(classnum).reshape(1, classnum)\n",
        "predict_label = np.zeros(classnum).reshape(1, classnum)\n",
        "\n",
        "file_name = '_stft.txt'\n",
        "\n",
        "start_row = 46\n",
        "end_row = 82\n",
        "scale_row = 1\n",
        "rows = 128\n",
        "\n",
        "start_col = 1\n",
        "end_col = 29\n",
        "scale_col = 1\n",
        "cols = 29\n",
        "\n",
        "# function for creating CNN model\n",
        "def create_CNNmodel(img_row, img_col, classnum):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=(img_row, img_col, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 1:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 2:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 3:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 4:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "def preprocessing(motion):\n",
        "    DirectoryPath = '/content/drive/MyDrive/dataset/'\n",
        "    image = np.zeros(shape=(count, rows, cols, 1))\n",
        "    label = []\n",
        "\n",
        "    for person in range(1, 5):\n",
        "        cwt_data = pd.read_csv(\n",
        "            DirectoryPath + date + \"_\" + str(person) + \"_\" + str(motion) + file_name)\n",
        "        for i in range(0, 100):\n",
        "            df = np.fromstring(cwt_data['pixels'][i], dtype=int, sep=' ')\n",
        "            df = np.reshape(df, (rows, cols, 1))\n",
        "            image[i + 100*(person-1)] = df\n",
        "            if motion == 0:\n",
        "                label.append(motion)\n",
        "            else:\n",
        "                label.append(motion-1)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# 시작과 끝 좌표는 scale한 후의 좌표를 기준으로 함\n",
        "def preprocessing_resize_crop(image, start_row, end_row, start_col, end_col, row_scale, col_scale):\n",
        "    crop_image = image[:, 0:image.shape[1]                       :row_scale, 0:image.shape[2]:col_scale]\n",
        "    crop_image = crop_image[:, start_row:end_row, start_col:end_col]\n",
        "    return crop_image\n",
        "\n",
        "\n",
        "# ratio비율로 각 data set을 합치고 순서도 섞음\n",
        "def concatenate_n_div(image0, label0, image1, label1, image2, label2):\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15  # 적용안됨\n",
        "\n",
        "    x_train = np.concatenate(\n",
        "        (image0[0:int(count*train_ratio)], image1[0:int(count*train_ratio)], image2[0:int(count*train_ratio)]))\n",
        "    y_train = np.concatenate(\n",
        "        (label0[0:int(count*train_ratio)], label1[0:int(count*train_ratio)], label2[0:int(count*train_ratio)]))\n",
        "    x_val = np.concatenate((image0[int(count*train_ratio):int(count*train_ratio + count*val_ratio)],\n",
        "                            image1[int(count*train_ratio):int(count*train_ratio + count*val_ratio)],\n",
        "                            image2[int(count*train_ratio):int(count*train_ratio + count*val_ratio)]))\n",
        "    y_val = np.concatenate((label0[int(count*train_ratio):int(count*train_ratio + count*val_ratio)],\n",
        "                            label1[int(count*train_ratio):int(count*train_ratio + count*val_ratio)],\n",
        "                            label2[int(count*train_ratio):int(count*train_ratio + count*val_ratio)]))\n",
        "    x_test = np.concatenate((image0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             image1[int(count*train_ratio + count*val_ratio): count],\n",
        "                             image2[int(count*train_ratio + count*val_ratio): count]))\n",
        "    y_test = np.concatenate((label0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             label1[int(count*train_ratio + count*val_ratio): count],\n",
        "                             label2[int(count*train_ratio + count*val_ratio): count]))\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        width_shift_range=wsr\n",
        "    )\n",
        "\n",
        "    augment_size = int(augment_ratio * x_train.shape[0])\n",
        "    randidx = np.random.randint(x_train.shape[0], size=augment_size)\n",
        "    x_augmented = x_train[randidx].copy()\n",
        "    y_augmented = y_train[randidx].copy()\n",
        "    x_augmented, y_augmented = train_gen.flow(\n",
        "        x_augmented, y_augmented,  batch_size=augment_size, shuffle=False).next()\n",
        "    x_train = np.concatenate((x_train, x_augmented))\n",
        "    y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "    s = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_train = x_train[s]\n",
        "    y_train = y_train[s]\n",
        "\n",
        "    s = np.arange(x_val.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_val = x_val[s]\n",
        "    y_val = y_val[s]\n",
        "\n",
        "    s = np.arange(x_test.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_test = x_test[s]\n",
        "    y_test = y_test[s]\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "\n",
        "row_len = math.ceil((end_row - start_row))\n",
        "col_len = math.ceil((end_col - start_col))\n",
        "\n",
        "image1, label1 = preprocessing(0)  # motion 0 불러옴\n",
        "image2, label2 = preprocessing(2)  # motion 2 불러옴\n",
        "image3, label3 = preprocessing(3)  # motion 3 불러옴\n",
        "\n",
        "result_acc = 0\n",
        "\n",
        "for i in range(try_num):\n",
        "    print(str(i + 1) + ' repeat')\n",
        "\n",
        "    # 순서를 섞음\n",
        "    s = np.arange(image1.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image1_shuff = image1[s]\n",
        "\n",
        "    s = np.arange(image2.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image2_shuff = image2[s]\n",
        "\n",
        "    s = np.arange(image3.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image3_shuff = image3[s]\n",
        "\n",
        "    # 크기에 맞게 자름\n",
        "    image1_crop = preprocessing_resize_crop(\n",
        "        image1_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image2_crop = preprocessing_resize_crop(\n",
        "        image2_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image3_crop = preprocessing_resize_crop(\n",
        "        image3_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "\n",
        "    # 자른 image를 각 data set으로 나눠서 합침\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = concatenate_n_div(\n",
        "        image1_crop, label1, image2_crop, label2, image3_crop, label3)\n",
        "\n",
        "    maxval = x_train.max()\n",
        "    if maxval < x_val.max():\n",
        "        maxval = x_val.max()\n",
        "    if maxval < x_test.max():\n",
        "        maxval = x_test.max()\n",
        "\n",
        "    # 정규화\n",
        "    x_train = x_train.astype('float32')/maxval\n",
        "    x_val = x_val.astype('float32')/maxval\n",
        "    x_test = x_test.astype('float32')/maxval\n",
        "\n",
        "    model = create_CNNmodel(row_len, col_len, classnum)\n",
        "    if i == 0:\n",
        "        print(model.summary())\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "    y_train = np_utils.to_categorical(y_train, classnum)\n",
        "    y_val = np_utils.to_categorical(y_val, classnum)\n",
        "    y_test = np_utils.to_categorical(y_test, classnum)\n",
        "\n",
        "    # CNN 훈련\n",
        "    hist = model.fit(x_train, y_train, validation_data=(\n",
        "        x_val, y_val), epochs=50, callbacks=[early_stopping], verbose=0, batch_size=bs)\n",
        "\n",
        "    # 평가\n",
        "    # print('Evaluate')\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    # print('Test loss:', score[0])\n",
        "    # print('Test accuracy:', score[1])\n",
        "    result_acc = result_acc + score[1]    # 정확도 결과 저장하여 평균값 내는데 사용\n",
        "\n",
        "    test_label = np.concatenate((test_label, y_test))\n",
        "    pred = model.predict(x_test)\n",
        "    predict_label = np.concatenate((predict_label, pred))\n",
        "\n",
        "print('image size :', str(row_len)+'X'+str(col_len), '   row =', str(start_row)+' : '+str(end_row), '   col =', str(start_col)+' : '+str(end_col),\n",
        "      '   round :', try_num, '//  average_acc :', result_acc / try_num)\n",
        "\n",
        "test_label = np.delete(test_label, 0, axis=0)\n",
        "predict_label = np.delete(predict_label, 0, axis=0)\n",
        "sns.set(style='white')\n",
        "plt.figure(figsize=(classnum, classnum))\n",
        "cm = confusion_matrix(np.argmax(test_label[:int(test_label.shape[0])], axis=1),\n",
        "                      np.argmax(predict_label[:int(predict_label.shape[0])], axis=-1))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Classify Motion')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(model.summary())\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "s7lmg6MkkzvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "motion = 0\n",
        "\n",
        "augment_ratio = 9\n",
        "classnum = 4     # class 개수\n",
        "try_num = 30   # 같은 조건에서 몇번 반복할지\n",
        "\n",
        "date = '220132'\n",
        "count = 100\n",
        "\n",
        "lr = 0.001\n",
        "bs = 64\n",
        "wsr = 0.15\n",
        "\n",
        "test_label = np.zeros(classnum).reshape(1, classnum)\n",
        "predict_label = np.zeros(classnum).reshape(1, classnum)\n",
        "\n",
        "file_name = '_stft.txt'\n",
        "\n",
        "start_row = 46\n",
        "end_row = 82\n",
        "scale_row = 1\n",
        "rows = 128\n",
        "\n",
        "start_col = 1\n",
        "end_col = 29\n",
        "scale_col = 1\n",
        "cols = 29\n",
        "\n",
        "\n",
        "# function for creating CNN model\n",
        "def create_CNNmodel(img_row, img_col, classnum):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=(img_row, img_col, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 1:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 2:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 3:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 4:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "\n",
        "def preprocessing(person, motion):  # person, motion에 해당하는 image 불러옴\n",
        "    DirectoryPath = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "    image = np.zeros(shape=(count, rows, cols, 1))\n",
        "    label = []\n",
        "    cwt_data = pd.read_csv(\n",
        "        DirectoryPath + date + \"_\" + str(person) + \"_\" + str(motion) + file_name)\n",
        "    for i in range(0, count):\n",
        "        df = np.fromstring(cwt_data['pixels'][i], dtype=int, sep=' ')\n",
        "        df = np.reshape(df, (rows, cols, 1))\n",
        "        image[i] = df\n",
        "        label.append(person - 1)    # 사람으로 구분\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# 시작과 끝 좌표는 scale한 후의 좌표를 기준으로 함\n",
        "def preprocessing_resize_crop(image, start_row, end_row, start_col, end_col, row_scale, col_scale):\n",
        "    crop_image = image[:, 0:image.shape[1]                       :row_scale, 0:image.shape[2]:col_scale]\n",
        "    crop_image = crop_image[:, start_row:end_row, start_col:end_col]\n",
        "    return crop_image\n",
        "\n",
        "\n",
        "# ratio비율로 각 data set을 합치고 순서도 섞음\n",
        "def concatenate_n_div(image0, label0, image1, label1, image2, label2, image3, label3):\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15  # 적용안됨\n",
        "\n",
        "    x_train = np.concatenate((image0[0:int(count*train_ratio)], image1[0:int(\n",
        "        count*train_ratio)], image2[0:int(count*train_ratio)], image3[0:int(count*train_ratio)]))\n",
        "    y_train = np.concatenate((label0[0:int(count*train_ratio)], label1[0:int(\n",
        "        count*train_ratio)], label2[0:int(count*train_ratio)], label3[0:int(count*train_ratio)]))\n",
        "    x_val = np.concatenate((image0[int(count*train_ratio): int(count*train_ratio + count*val_ratio)],\n",
        "                            image1[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            image2[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            image3[int(count*train_ratio): int(count*train_ratio + count*val_ratio)]))\n",
        "    y_val = np.concatenate((label0[int(count*train_ratio): int(count*train_ratio + count*val_ratio)],\n",
        "                            label1[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            label2[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            label3[int(count*train_ratio): int(count*train_ratio + count*val_ratio)]))\n",
        "    x_test = np.concatenate((image0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             image1[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             image2[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             image3[int(count*train_ratio + count*val_ratio): count]))\n",
        "    y_test = np.concatenate((label0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             label1[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             label2[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             label3[int(count*train_ratio + count*val_ratio): count]))\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        width_shift_range=wsr\n",
        "    )\n",
        "\n",
        "    augment_size = int(augment_ratio * x_train.shape[0])\n",
        "    randidx = np.random.randint(x_train.shape[0], size=augment_size)\n",
        "    x_augmented = x_train[randidx].copy()\n",
        "    y_augmented = y_train[randidx].copy()\n",
        "    x_augmented, y_augmented = train_gen.flow(\n",
        "        x_augmented, y_augmented,  batch_size=augment_size, shuffle=False).next()\n",
        "    x_train = np.concatenate((x_train, x_augmented))\n",
        "    y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "    s = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_train = x_train[s]\n",
        "    y_train = y_train[s]\n",
        "\n",
        "    s = np.arange(x_val.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_val = x_val[s]\n",
        "    y_val = y_val[s]\n",
        "\n",
        "    s = np.arange(x_test.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_test = x_test[s]\n",
        "    y_test = y_test[s]\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "\n",
        "row_len = math.ceil((end_row - start_row))\n",
        "col_len = math.ceil((end_col - start_col))\n",
        "\n",
        "\n",
        "image1, label1 = preprocessing(1, motion)  # 성진_motion 불러옴\n",
        "image2, label2 = preprocessing(2, motion)  # 호정_motion 불러옴\n",
        "image3, label3 = preprocessing(3, motion)  # 여성1_motion 불러옴\n",
        "image4, label4 = preprocessing(4, motion)  # 여성2_motion 불러옴\n",
        "\n",
        "model = create_CNNmodel(row_len, col_len, classnum)\n",
        "\n",
        "result_acc = 0\n",
        "for i in range(try_num):\n",
        "    print(str(i + 1) + ' repeat')\n",
        "\n",
        "    # 순서를 섞음\n",
        "    s = np.arange(image1.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image1_shuff = image1[s]\n",
        "\n",
        "    s = np.arange(image2.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image2_shuff = image2[s]\n",
        "\n",
        "    s = np.arange(image3.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image3_shuff = image3[s]\n",
        "\n",
        "    s = np.arange(image4.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image4_shuff = image4[s]\n",
        "\n",
        "    # 크기에 맞게 자름\n",
        "    image1_crop = preprocessing_resize_crop(\n",
        "        image1_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image2_crop = preprocessing_resize_crop(\n",
        "        image2_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image3_crop = preprocessing_resize_crop(\n",
        "        image3_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image4_crop = preprocessing_resize_crop(\n",
        "        image4_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "\n",
        "    # 자른 image를 각 data set으로 나눠서 합침\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = concatenate_n_div(\n",
        "        image1_crop, label1, image2_crop, label2, image3_crop, label3, image4_crop, label4)\n",
        "\n",
        "    maxval = x_train.max()\n",
        "    if maxval < x_val.max():\n",
        "        maxval = x_val.max()\n",
        "    if maxval < x_test.max():\n",
        "        maxval = x_test.max()\n",
        "\n",
        "    # 정규화\n",
        "    x_train = x_train.astype('float32')/maxval\n",
        "    x_val = x_val.astype('float32')/maxval\n",
        "    x_test = x_test.astype('float32')/maxval\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "    y_train = np_utils.to_categorical(y_train, classnum)\n",
        "    y_val = np_utils.to_categorical(y_val, classnum)\n",
        "    y_test = np_utils.to_categorical(y_test, classnum)\n",
        "\n",
        "    # CNN 훈련\n",
        "    hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, callbacks=[early_stopping], verbose=0, batch_size=bs)\n",
        "\n",
        "    # 평가\n",
        "    # print('Evaluate')\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    # print('Test loss:', score[0])\n",
        "    # print('Test accuracy:', score[1])\n",
        "    result_acc = result_acc + score[1]    # 정확도 결과 저장하여 평균값 내는데 사용\n",
        "\n",
        "    test_label = np.concatenate((test_label,y_test))\n",
        "    pred = model.predict(x_test)\n",
        "    predict_label = np.concatenate((predict_label,pred))\n",
        "\n",
        "print('image size :', str(row_len)+'X'+str(col_len), '   row =', str(start_row)+' : '+str(end_row), '   col =', str(start_col)+' : '+str(end_col),\n",
        "      '   round :', try_num, '//  average_acc :', result_acc / try_num)\n",
        "\n",
        "\n",
        "test_label = np.delete(test_label,0,axis=0)\n",
        "predict_label = np.delete(predict_label,0,axis=0)\n",
        "sns.set(style='white')\n",
        "plt.figure(figsize=(classnum,classnum))\n",
        "cm = confusion_matrix(np.argmax(test_label[:int(test_label.shape[0])], axis=1),\n",
        "np.argmax(predict_label[:int(predict_label.shape[0])], axis=-1))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "if motion == 0:\n",
        "    plt.title('Classify Human in Walk')\n",
        "elif motion == 2:\n",
        "    plt.title('Classify Human in Stride')\n",
        "else:\n",
        "    plt.title('Classify Human in Creep')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(model.summary())"
      ],
      "outputs": [],
      "metadata": {
        "id": "6uZOiyfgkans"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "motion = 2\n",
        "\n",
        "augment_ratio = 9\n",
        "classnum = 4     # class 개수\n",
        "try_num = 30   # 같은 조건에서 몇번 반복할지\n",
        "\n",
        "date = '220132'\n",
        "count = 100\n",
        "\n",
        "lr = 0.001\n",
        "bs = 64\n",
        "wsr = 0.15\n",
        "\n",
        "test_label = np.zeros(classnum).reshape(1, classnum)\n",
        "predict_label = np.zeros(classnum).reshape(1, classnum)\n",
        "\n",
        "file_name = '_stft.txt'\n",
        "\n",
        "start_row = 46\n",
        "end_row = 82\n",
        "scale_row = 1\n",
        "rows = 128\n",
        "\n",
        "start_col = 1\n",
        "end_col = 29\n",
        "scale_col = 1\n",
        "cols = 29\n",
        "\n",
        "\n",
        "# function for creating CNN model\n",
        "def create_CNNmodel(img_row, img_col, classnum):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=(img_row, img_col, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 1:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 2:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 3:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 4:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "\n",
        "def preprocessing(person, motion):  # person, motion에 해당하는 image 불러옴\n",
        "    DirectoryPath = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "    image = np.zeros(shape=(count, rows, cols, 1))\n",
        "    label = []\n",
        "    cwt_data = pd.read_csv(\n",
        "        DirectoryPath + date + \"_\" + str(person) + \"_\" + str(motion) + file_name)\n",
        "    for i in range(0, count):\n",
        "        df = np.fromstring(cwt_data['pixels'][i], dtype=int, sep=' ')\n",
        "        df = np.reshape(df, (rows, cols, 1))\n",
        "        image[i] = df\n",
        "        label.append(person - 1)    # 사람으로 구분\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# 시작과 끝 좌표는 scale한 후의 좌표를 기준으로 함\n",
        "def preprocessing_resize_crop(image, start_row, end_row, start_col, end_col, row_scale, col_scale):\n",
        "    crop_image = image[:, 0:image.shape[1]                       :row_scale, 0:image.shape[2]:col_scale]\n",
        "    crop_image = crop_image[:, start_row:end_row, start_col:end_col]\n",
        "    return crop_image\n",
        "\n",
        "\n",
        "# ratio비율로 각 data set을 합치고 순서도 섞음\n",
        "def concatenate_n_div(image0, label0, image1, label1, image2, label2, image3, label3):\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15  # 적용안됨\n",
        "\n",
        "    x_train = np.concatenate((image0[0:int(count*train_ratio)], image1[0:int(\n",
        "        count*train_ratio)], image2[0:int(count*train_ratio)], image3[0:int(count*train_ratio)]))\n",
        "    y_train = np.concatenate((label0[0:int(count*train_ratio)], label1[0:int(\n",
        "        count*train_ratio)], label2[0:int(count*train_ratio)], label3[0:int(count*train_ratio)]))\n",
        "    x_val = np.concatenate((image0[int(count*train_ratio): int(count*train_ratio + count*val_ratio)],\n",
        "                            image1[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            image2[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            image3[int(count*train_ratio): int(count*train_ratio + count*val_ratio)]))\n",
        "    y_val = np.concatenate((label0[int(count*train_ratio): int(count*train_ratio + count*val_ratio)],\n",
        "                            label1[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            label2[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            label3[int(count*train_ratio): int(count*train_ratio + count*val_ratio)]))\n",
        "    x_test = np.concatenate((image0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             image1[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             image2[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             image3[int(count*train_ratio + count*val_ratio): count]))\n",
        "    y_test = np.concatenate((label0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             label1[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             label2[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             label3[int(count*train_ratio + count*val_ratio): count]))\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        width_shift_range=wsr\n",
        "    )\n",
        "\n",
        "    augment_size = int(augment_ratio * x_train.shape[0])\n",
        "    randidx = np.random.randint(x_train.shape[0], size=augment_size)\n",
        "    x_augmented = x_train[randidx].copy()\n",
        "    y_augmented = y_train[randidx].copy()\n",
        "    x_augmented, y_augmented = train_gen.flow(\n",
        "        x_augmented, y_augmented,  batch_size=augment_size, shuffle=False).next()\n",
        "    x_train = np.concatenate((x_train, x_augmented))\n",
        "    y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "    s = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_train = x_train[s]\n",
        "    y_train = y_train[s]\n",
        "\n",
        "    s = np.arange(x_val.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_val = x_val[s]\n",
        "    y_val = y_val[s]\n",
        "\n",
        "    s = np.arange(x_test.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_test = x_test[s]\n",
        "    y_test = y_test[s]\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "\n",
        "row_len = math.ceil((end_row - start_row))\n",
        "col_len = math.ceil((end_col - start_col))\n",
        "\n",
        "\n",
        "image1, label1 = preprocessing(1, motion)  # 성진_motion 불러옴\n",
        "image2, label2 = preprocessing(2, motion)  # 호정_motion 불러옴\n",
        "image3, label3 = preprocessing(3, motion)  # 여성1_motion 불러옴\n",
        "image4, label4 = preprocessing(4, motion)  # 여성2_motion 불러옴\n",
        "\n",
        "model = create_CNNmodel(row_len, col_len, classnum)\n",
        "\n",
        "result_acc = 0\n",
        "for i in range(try_num):\n",
        "    print(str(i + 1) + ' repeat')\n",
        "\n",
        "    # 순서를 섞음\n",
        "    s = np.arange(image1.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image1_shuff = image1[s]\n",
        "\n",
        "    s = np.arange(image2.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image2_shuff = image2[s]\n",
        "\n",
        "    s = np.arange(image3.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image3_shuff = image3[s]\n",
        "\n",
        "    s = np.arange(image4.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image4_shuff = image4[s]\n",
        "\n",
        "    # 크기에 맞게 자름\n",
        "    image1_crop = preprocessing_resize_crop(\n",
        "        image1_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image2_crop = preprocessing_resize_crop(\n",
        "        image2_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image3_crop = preprocessing_resize_crop(\n",
        "        image3_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image4_crop = preprocessing_resize_crop(\n",
        "        image4_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "\n",
        "    # 자른 image를 각 data set으로 나눠서 합침\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = concatenate_n_div(\n",
        "        image1_crop, label1, image2_crop, label2, image3_crop, label3, image4_crop, label4)\n",
        "\n",
        "    maxval = x_train.max()\n",
        "    if maxval < x_val.max():\n",
        "        maxval = x_val.max()\n",
        "    if maxval < x_test.max():\n",
        "        maxval = x_test.max()\n",
        "\n",
        "    # 정규화\n",
        "    x_train = x_train.astype('float32')/maxval\n",
        "    x_val = x_val.astype('float32')/maxval\n",
        "    x_test = x_test.astype('float32')/maxval\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "    y_train = np_utils.to_categorical(y_train, classnum)\n",
        "    y_val = np_utils.to_categorical(y_val, classnum)\n",
        "    y_test = np_utils.to_categorical(y_test, classnum)\n",
        "\n",
        "    # CNN 훈련\n",
        "    hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, callbacks=[early_stopping], verbose=0, batch_size=bs)\n",
        "\n",
        "    # 평가\n",
        "    # print('Evaluate')\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    # print('Test loss:', score[0])\n",
        "    # print('Test accuracy:', score[1])\n",
        "    result_acc = result_acc + score[1]    # 정확도 결과 저장하여 평균값 내는데 사용\n",
        "\n",
        "    test_label = np.concatenate((test_label,y_test))\n",
        "    pred = model.predict(x_test)\n",
        "    predict_label = np.concatenate((predict_label,pred))\n",
        "\n",
        "print('image size :', str(row_len)+'X'+str(col_len), '   row =', str(start_row)+' : '+str(end_row), '   col =', str(start_col)+' : '+str(end_col),\n",
        "      '   round :', try_num, '//  average_acc :', result_acc / try_num)\n",
        "\n",
        "\n",
        "test_label = np.delete(test_label,0,axis=0)\n",
        "predict_label = np.delete(predict_label,0,axis=0)\n",
        "sns.set(style='white')\n",
        "plt.figure(figsize=(classnum,classnum))\n",
        "cm = confusion_matrix(np.argmax(test_label[:int(test_label.shape[0])], axis=1),\n",
        "np.argmax(predict_label[:int(predict_label.shape[0])], axis=-1))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "if motion == 0:\n",
        "    plt.title('Classify Human in Walk')\n",
        "elif motion == 2:\n",
        "    plt.title('Classify Human in Stride')\n",
        "else:\n",
        "    plt.title('Classify Human in Creep')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(model.summary())"
      ],
      "outputs": [],
      "metadata": {
        "id": "jLkuMKa8l_KM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "motion = 3\n",
        "\n",
        "augment_ratio = 9\n",
        "classnum = 4     # class 개수\n",
        "try_num = 30   # 같은 조건에서 몇번 반복할지\n",
        "\n",
        "date = '220132'\n",
        "count = 100\n",
        "\n",
        "lr = 0.001\n",
        "bs = 64\n",
        "wsr = 0.15\n",
        "\n",
        "test_label = np.zeros(classnum).reshape(1, classnum)\n",
        "predict_label = np.zeros(classnum).reshape(1, classnum)\n",
        "\n",
        "file_name = '_stft.txt'\n",
        "\n",
        "start_row = 46\n",
        "end_row = 82\n",
        "scale_row = 1\n",
        "rows = 128\n",
        "\n",
        "start_col = 1\n",
        "end_col = 29\n",
        "scale_col = 1\n",
        "cols = 29\n",
        "\n",
        "\n",
        "# function for creating CNN model\n",
        "def create_CNNmodel(img_row, img_col, classnum):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu', input_shape=(img_row, img_col, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 1:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 2:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 3:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1,1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "    if convlayers == 4:\n",
        "          model.add(Flatten())\n",
        "          model.add(Dense(denseparam, activation='relu'))\n",
        "          model.add(Dense(classnum, activation='softmax'))\n",
        "          model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=Adam(lr), metrics=['accuracy'])\n",
        "          return model\n",
        "\n",
        "\n",
        "def preprocessing(person, motion):  # person, motion에 해당하는 image 불러옴\n",
        "    DirectoryPath = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "    image = np.zeros(shape=(count, rows, cols, 1))\n",
        "    label = []\n",
        "    cwt_data = pd.read_csv(\n",
        "        DirectoryPath + date + \"_\" + str(person) + \"_\" + str(motion) + file_name)\n",
        "    for i in range(0, count):\n",
        "        df = np.fromstring(cwt_data['pixels'][i], dtype=int, sep=' ')\n",
        "        df = np.reshape(df, (rows, cols, 1))\n",
        "        image[i] = df\n",
        "        label.append(person - 1)    # 사람으로 구분\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# 시작과 끝 좌표는 scale한 후의 좌표를 기준으로 함\n",
        "def preprocessing_resize_crop(image, start_row, end_row, start_col, end_col, row_scale, col_scale):\n",
        "    crop_image = image[:, 0:image.shape[1]                       :row_scale, 0:image.shape[2]:col_scale]\n",
        "    crop_image = crop_image[:, start_row:end_row, start_col:end_col]\n",
        "    return crop_image\n",
        "\n",
        "\n",
        "# ratio비율로 각 data set을 합치고 순서도 섞음\n",
        "def concatenate_n_div(image0, label0, image1, label1, image2, label2, image3, label3):\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15  # 적용안됨\n",
        "\n",
        "    x_train = np.concatenate((image0[0:int(count*train_ratio)], image1[0:int(\n",
        "        count*train_ratio)], image2[0:int(count*train_ratio)], image3[0:int(count*train_ratio)]))\n",
        "    y_train = np.concatenate((label0[0:int(count*train_ratio)], label1[0:int(\n",
        "        count*train_ratio)], label2[0:int(count*train_ratio)], label3[0:int(count*train_ratio)]))\n",
        "    x_val = np.concatenate((image0[int(count*train_ratio): int(count*train_ratio + count*val_ratio)],\n",
        "                            image1[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            image2[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            image3[int(count*train_ratio): int(count*train_ratio + count*val_ratio)]))\n",
        "    y_val = np.concatenate((label0[int(count*train_ratio): int(count*train_ratio + count*val_ratio)],\n",
        "                            label1[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            label2[int(count*train_ratio): int(count *\n",
        "                                                               train_ratio + count*val_ratio)],\n",
        "                            label3[int(count*train_ratio): int(count*train_ratio + count*val_ratio)]))\n",
        "    x_test = np.concatenate((image0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             image1[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             image2[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             image3[int(count*train_ratio + count*val_ratio): count]))\n",
        "    y_test = np.concatenate((label0[int(count*train_ratio + count*val_ratio): count],\n",
        "                             label1[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             label2[int(count*train_ratio +\n",
        "                                        count*val_ratio): count],\n",
        "                             label3[int(count*train_ratio + count*val_ratio): count]))\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        width_shift_range=wsr\n",
        "    )\n",
        "\n",
        "    augment_size = int(augment_ratio * x_train.shape[0])\n",
        "    randidx = np.random.randint(x_train.shape[0], size=augment_size)\n",
        "    x_augmented = x_train[randidx].copy()\n",
        "    y_augmented = y_train[randidx].copy()\n",
        "    x_augmented, y_augmented = train_gen.flow(\n",
        "        x_augmented, y_augmented,  batch_size=augment_size, shuffle=False).next()\n",
        "    x_train = np.concatenate((x_train, x_augmented))\n",
        "    y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "    s = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_train = x_train[s]\n",
        "    y_train = y_train[s]\n",
        "\n",
        "    s = np.arange(x_val.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_val = x_val[s]\n",
        "    y_val = y_val[s]\n",
        "\n",
        "    s = np.arange(x_test.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_test = x_test[s]\n",
        "    y_test = y_test[s]\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "\n",
        "row_len = math.ceil((end_row - start_row))\n",
        "col_len = math.ceil((end_col - start_col))\n",
        "\n",
        "\n",
        "image1, label1 = preprocessing(1, motion)  # 성진_motion 불러옴\n",
        "image2, label2 = preprocessing(2, motion)  # 호정_motion 불러옴\n",
        "image3, label3 = preprocessing(3, motion)  # 여성1_motion 불러옴\n",
        "image4, label4 = preprocessing(4, motion)  # 여성2_motion 불러옴\n",
        "\n",
        "model = create_CNNmodel(row_len, col_len, classnum)\n",
        "\n",
        "result_acc = 0\n",
        "for i in range(try_num):\n",
        "    print(str(i + 1) + ' repeat')\n",
        "\n",
        "    # 순서를 섞음\n",
        "    s = np.arange(image1.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image1_shuff = image1[s]\n",
        "\n",
        "    s = np.arange(image2.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image2_shuff = image2[s]\n",
        "\n",
        "    s = np.arange(image3.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image3_shuff = image3[s]\n",
        "\n",
        "    s = np.arange(image4.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image4_shuff = image4[s]\n",
        "\n",
        "    # 크기에 맞게 자름\n",
        "    image1_crop = preprocessing_resize_crop(\n",
        "        image1_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image2_crop = preprocessing_resize_crop(\n",
        "        image2_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image3_crop = preprocessing_resize_crop(\n",
        "        image3_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image4_crop = preprocessing_resize_crop(\n",
        "        image4_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "\n",
        "    # 자른 image를 각 data set으로 나눠서 합침\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = concatenate_n_div(\n",
        "        image1_crop, label1, image2_crop, label2, image3_crop, label3, image4_crop, label4)\n",
        "\n",
        "    maxval = x_train.max()\n",
        "    if maxval < x_val.max():\n",
        "        maxval = x_val.max()\n",
        "    if maxval < x_test.max():\n",
        "        maxval = x_test.max()\n",
        "\n",
        "    # 정규화\n",
        "    x_train = x_train.astype('float32')/maxval\n",
        "    x_val = x_val.astype('float32')/maxval\n",
        "    x_test = x_test.astype('float32')/maxval\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "    y_train = np_utils.to_categorical(y_train, classnum)\n",
        "    y_val = np_utils.to_categorical(y_val, classnum)\n",
        "    y_test = np_utils.to_categorical(y_test, classnum)\n",
        "\n",
        "    # CNN 훈련\n",
        "    hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, callbacks=[early_stopping], verbose=0, batch_size=bs)\n",
        "\n",
        "    # 평가\n",
        "    # print('Evaluate')\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    # print('Test loss:', score[0])\n",
        "    # print('Test accuracy:', score[1])\n",
        "    result_acc = result_acc + score[1]    # 정확도 결과 저장하여 평균값 내는데 사용\n",
        "\n",
        "    test_label = np.concatenate((test_label,y_test))\n",
        "    pred = model.predict(x_test)\n",
        "    predict_label = np.concatenate((predict_label,pred))\n",
        "\n",
        "print('image size :', str(row_len)+'X'+str(col_len), '   row =', str(start_row)+' : '+str(end_row), '   col =', str(start_col)+' : '+str(end_col),\n",
        "      '   round :', try_num, '//  average_acc :', result_acc / try_num)\n",
        "\n",
        "\n",
        "test_label = np.delete(test_label,0,axis=0)\n",
        "predict_label = np.delete(predict_label,0,axis=0)\n",
        "sns.set(style='white')\n",
        "plt.figure(figsize=(classnum,classnum))\n",
        "cm = confusion_matrix(np.argmax(test_label[:int(test_label.shape[0])], axis=1),\n",
        "np.argmax(predict_label[:int(predict_label.shape[0])], axis=-1))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "if motion == 0:\n",
        "    plt.title('Classify Human in Walk')\n",
        "elif motion == 2:\n",
        "    plt.title('Classify Human in Stride')\n",
        "else:\n",
        "    plt.title('Classify Human in Creep')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(model.summary())"
      ],
      "outputs": [],
      "metadata": {
        "id": "_TTU22HGmA4a"
      }
    }
  ]
}