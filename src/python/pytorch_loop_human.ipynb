{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOIquSZ17Pni",
        "outputId": "cd218212-9fec-437a-af2d-63583802dab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i0Wbira93jv"
      },
      "source": [
        "# Module Setting\n",
        "![image.png](data:image/png;base64,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)\n",
        "pytorchtools, binarized_modules import필요 \n",
        "다른 방법으로 해도 무방"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ehfon_SG6tzd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as tr\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from pytorchtools import EarlyStopping\n",
        "from pytorchtools import EarlyStopping_acc\n",
        "from torchsummary import summary as summary_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Function\n",
        "from binarized_modules import  BinarizeLinear,BinarizeConv2d\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M5DIAy_l6tzf"
      },
      "outputs": [],
      "source": [
        "def preprocessing(person,motion): # person, motion에 해당하는 image 불러옴\n",
        "    date = '220132'\n",
        "#    file_name = '_cwt.txt'\n",
        "    file_name = '_stft.txt'\n",
        "    DirectoryPath = '/content/drive/MyDrive/data/class12/'\n",
        "    whole_count = 100\n",
        "#    image = np.zeros(shape=(whole_count, 81, 1920, 1))\n",
        "    image = np.zeros(shape=(whole_count, 128, 29, 1))\n",
        "    label = []\n",
        "    cwt_data = pd.read_csv(\n",
        "                DirectoryPath + date + \"_\" + str(person) + \"_\" + str(motion) + file_name)\n",
        "    for i in range(0, whole_count):\n",
        "        df = np.fromstring(cwt_data['pixels'][i], dtype=int, sep=' ')\n",
        "#        df = np.reshape(df, (81, 1920, 1))\n",
        "        df = np.reshape(df, (128, 29, 1))\n",
        "        image[i] = df\n",
        "        label.append(person-1)    # 사람으로 구분\n",
        "    return image, label\n",
        "\n",
        "\n",
        "\n",
        "# 시작과 끝 좌표는 scale한 후의 좌표를 기준으로 함\n",
        "def preprocessing_resize_crop(image,start_row,end_row,start_col,end_col,row_scale,col_scale): \n",
        "    crop_image = image[:,0:image.shape[1]:row_scale,0:image.shape[2]:col_scale]\n",
        "    crop_image = crop_image[:,start_row:end_row,start_col:end_col]\n",
        "    return crop_image\n",
        "\n",
        "def concatenate_n_div(image0, label0, image1, label1, image2, label2, image3, label3): # ratio비율로 각 data set을 합치고 순서도 섞음\n",
        "    \n",
        "\n",
        "    count = 100\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15 # 적용안됨\n",
        "    \n",
        "    x_train = np.concatenate((image0[0:int(count*train_ratio)], image1[0:int(count*train_ratio)], image2[0:int(count*train_ratio)],\n",
        "                              image3[0:int(count*train_ratio)]))\n",
        "    y_train = np.concatenate((label0[0:int(count*train_ratio)], label1[0:int(count*train_ratio)], label2[0:int(count*train_ratio)],\n",
        "                              label3[0:int(count*train_ratio)]))\n",
        "    x_val = np.concatenate((image0[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)],\n",
        "                            image1[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)],\n",
        "                            image2[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)],\n",
        "                            image3[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)]))\n",
        "    y_val = np.concatenate((label0[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)],\n",
        "                            label1[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)],\n",
        "                            label2[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)],\n",
        "                            label3[int(count*train_ratio) : int(count*train_ratio + count*val_ratio)]))\n",
        "    x_test = np.concatenate((image0[int(count*train_ratio + count*val_ratio) : count],\n",
        "                             image1[int(count*train_ratio + count*val_ratio) : count],\n",
        "                             image2[int(count*train_ratio + count*val_ratio) : count],\n",
        "                             image3[int(count*train_ratio + count*val_ratio) : count]))\n",
        "    y_test = np.concatenate((label0[int(count*train_ratio + count*val_ratio) : count],\n",
        "                             label1[int(count*train_ratio + count*val_ratio) : count],\n",
        "                             label2[int(count*train_ratio + count*val_ratio) : count],\n",
        "                             label3[int(count*train_ratio + count*val_ratio) : count]))\n",
        "    \n",
        "    s = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_train = x_train[s]\n",
        "    y_train = y_train[s]\n",
        "\n",
        "    s = np.arange(x_val.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_val = x_val[s]\n",
        "    y_val = y_val[s]\n",
        "\n",
        "    s = np.arange(x_test.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    x_test = x_test[s]\n",
        "    y_test = y_test[s]\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "class TensorData(Dataset):\n",
        "    \n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x_data = torch.FloatTensor(x_data)\n",
        "        self.x_data = self.x_data.permute(0,3,1,2) # 이미지 개수, 채널 수, 이미지 너비, 높이\n",
        "        self.y_data = torch.LongTensor(y_data)\n",
        "        self.len = self.y_data.shape[0]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RoWbtv-m6tzh"
      },
      "outputs": [],
      "source": [
        "# SCALE:16 /1초 추출 + 0.2초 씩 이동 / row 30\n",
        "#=========================================================\n",
        "#================파라미터 설정==========================\n",
        "#============================================================\n",
        "# STFT = (128, 29) //  CWT = (81,1920)\n",
        "# 좌표는 스케일 이후 범위\n",
        "start_row = 46\n",
        "end_row = 82\n",
        "scale_row = 1\n",
        "rows = 128\n",
        "\n",
        "start_col = 0\n",
        "end_col = 28\n",
        "scale_col = 1\n",
        "cols = 29\n",
        "\n",
        "aug = 1  # 1이면 augmentation 0이면 X\n",
        "\n",
        "gen = ImageDataGenerator(\n",
        "                         width_shift_range=0.2\n",
        "                            )\n",
        "\n",
        "\n",
        "motion = 3  # 추출 모션\n",
        "\n",
        "image1,label1 = preprocessing(1,motion) # 성진_motion 불러옴\n",
        "image2,label2 = preprocessing(2,motion) # 호정_motion 불러옴\n",
        "image3,label3 = preprocessing(3,motion) # 경민_motion 불러옴\n",
        "image4,label4 = preprocessing(4,motion) # 경민_motion 불러옴\n",
        "\n",
        "# 정규화 추가\n",
        "total_img = np.concatenate((image1, image2, image3, image4))\n",
        "total_std = total_img.std()\n",
        "total_mean = total_img.mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "l84H3mxm6tzh"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(Net,self).__init__()\n",
        "        self.infl_ratio=3;\n",
        "        self.features = nn.Sequential(\n",
        "            # BinarizeConv2d(입력 채널수, 출력 채널수, 커널 사이즈, 스트라이드, 패딩)\n",
        "            BinarizeConv2d(1, 16*self.infl_ratio,kernel_size=3,stride=1,padding=0),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(입출력 채널수)\n",
        "            nn.BatchNorm2d(16*self.infl_ratio),\n",
        "            nn.Hardtanh(),\n",
        "\n",
        "            BinarizeConv2d(16*self.infl_ratio, 16*self.infl_ratio,kernel_size=3,stride=1,padding=0),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(16*self.infl_ratio),\n",
        "            nn.Hardtanh(),\n",
        "\n",
        "            BinarizeConv2d(16*self.infl_ratio, 16*self.infl_ratio,kernel_size=3,stride=1,padding=0),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(16*self.infl_ratio),\n",
        "            nn.Hardtanh(),\n",
        "\n",
        "            BinarizeConv2d(16*self.infl_ratio, 16,kernel_size=3,stride=1,padding=0),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Hardtanh()\n",
        "\n",
        "         )\n",
        "            \n",
        "        self.classifier = nn.Sequential(\n",
        "            # BinarizeLinear(입력 dense길이, 출력 dense길이)\n",
        "            BinarizeLinear(16*20*12,num_classes)\n",
        "            \n",
        "        )\n",
        "        self.softmax = nn.Softmax()\n",
        "                \n",
        "                \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1,16*20*12)\n",
        "        x = self.classifier(x)\n",
        "        return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP4Nfx2T6tzi",
        "outputId": "84967812-f70e-4ef5-e0c6-3ac174a338fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqMAYsmt6tzi",
        "outputId": "a70e3a80-8bc8-4325-ba26-a689c13d09f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (features): Sequential(\n",
            "    (0): BinarizeConv2d(1, 48, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Hardtanh(min_val=-1.0, max_val=1.0)\n",
            "    (4): BinarizeConv2d(48, 48, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Hardtanh(min_val=-1.0, max_val=1.0)\n",
            "    (8): BinarizeConv2d(48, 48, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (9): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): Hardtanh(min_val=-1.0, max_val=1.0)\n",
            "    (12): BinarizeConv2d(48, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (13): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): Hardtanh(min_val=-1.0, max_val=1.0)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): BinarizeLinear(in_features=3840, out_features=4, bias=True)\n",
            "  )\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = Net().to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMv8cf8K6tzj",
        "outputId": "1da505b0-347d-4e2a-9838-615b4643c914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "    BinarizeConv2d-1           [32, 48, 34, 26]             480\n",
            "         MaxPool2d-2           [32, 48, 32, 24]               0\n",
            "       BatchNorm2d-3           [32, 48, 32, 24]              96\n",
            "          Hardtanh-4           [32, 48, 32, 24]               0\n",
            "    BinarizeConv2d-5           [32, 48, 30, 22]          20,784\n",
            "         MaxPool2d-6           [32, 48, 28, 20]               0\n",
            "       BatchNorm2d-7           [32, 48, 28, 20]              96\n",
            "          Hardtanh-8           [32, 48, 28, 20]               0\n",
            "    BinarizeConv2d-9           [32, 48, 26, 18]          20,784\n",
            "        MaxPool2d-10           [32, 48, 24, 16]               0\n",
            "      BatchNorm2d-11           [32, 48, 24, 16]              96\n",
            "         Hardtanh-12           [32, 48, 24, 16]               0\n",
            "   BinarizeConv2d-13           [32, 16, 22, 14]           6,928\n",
            "        MaxPool2d-14           [32, 16, 20, 12]               0\n",
            "      BatchNorm2d-15           [32, 16, 20, 12]              32\n",
            "         Hardtanh-16           [32, 16, 20, 12]               0\n",
            "   BinarizeLinear-17                    [32, 4]          15,364\n",
            "          Softmax-18                    [32, 4]               0\n",
            "================================================================\n",
            "Total params: 64,660\n",
            "Trainable params: 64,660\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.12\n",
            "Forward/backward pass size (MB): 87.78\n",
            "Params size (MB): 0.25\n",
            "Estimated Total Size (MB): 88.15\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "summary_(net,(1,36,28),batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ysV0Gond6tzj"
      },
      "outputs": [],
      "source": [
        "args_cuda = not False and torch.cuda.is_available()\n",
        "# epoch_num : 훈련 한번에 epoch을 몇번 실행하는지\n",
        "def train_1(epoch_num):\n",
        "    early_stopping = EarlyStopping_acc(patience = patience, verbose = True) # 다른 함수로 EarlyStopping는 loss기준\n",
        "    arr_acc = []\n",
        "    for epoch in range(1,epoch_num + 1):\n",
        "        valid_losses = []\n",
        "        train_losses = [] \n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            if args_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            for p in list(model.parameters()):\n",
        "                if hasattr(p,'org'):\n",
        "                    p.data.copy_(p.org)\n",
        "            optimizer.step()\n",
        "            for p in list(model.parameters()):\n",
        "                if hasattr(p,'org'):\n",
        "                    p.org.copy_(p.data.clamp_(-1,1))\n",
        "\n",
        "            # if batch_idx % 10 == 0:\n",
        "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            #         100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "        # valid_data로 평가\n",
        "        correct =0\n",
        "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
        "            if args_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_losses.append(loss.item())\n",
        "            \n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        val_acc = 100. * correct / len(valid_loader.dataset)\n",
        "\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "\n",
        "\n",
        "        epoch_len = len(str(epoch_num))\n",
        "\n",
        "        print(f\"\\n현재 epoch : {epoch}\\n\")\n",
        "        print_msg = (f'[{epoch:>{epoch_len}}/{epoch_num:>{epoch_len}}] ' +\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_accuary: {val_acc:.1f}%')\n",
        "\n",
        "        print(print_msg)\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        # early_stopping는 validation acc가 감소하였는지 확인이 필요하며,\n",
        "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
        "        \n",
        "\n",
        "        # earlystopping \n",
        "        early_stopping(val_acc, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        if epoch%10==0:\n",
        "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*0.9  # 훈련이 진행됨에 따라 lr 감소\n",
        "        eval()\n",
        "    #model.load_state_dict(torch.load('checkpoint.pt')) # earlystopping이 끝나고 best모델을 load할지\n",
        "    test() # test data의 결과 추이도 보기 위해 추가함\n",
        "\n",
        "# test와 eavl 차이는 마지막에 정확도를 list에 추가하는지 여부만 다름\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            if args_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    arr_acc.append((100. * correct / len(test_loader.dataset)).item())\n",
        "\n",
        "def eval():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            if args_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7XURfA4y6tzk",
        "outputId": "10e663d7-e139-4258-88f1-f570720843ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 시도 횟수 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "현재 epoch : 1\n",
            "\n",
            "[  1/100] train_loss: 1.38493 valid_loss: 1.25311 valid_accuary: 48.3%\n",
            "Validation accuray increased (inf --> 48.333332).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0425, Accuracy: 28/60 (47%)\n",
            "\n",
            "\n",
            "현재 epoch : 2\n",
            "\n",
            "[  2/100] train_loss: 1.28875 valid_loss: 1.28016 valid_accuary: 46.7%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0435, Accuracy: 26/60 (43%)\n",
            "\n",
            "\n",
            "현재 epoch : 3\n",
            "\n",
            "[  3/100] train_loss: 1.28624 valid_loss: 1.23324 valid_accuary: 50.0%\n",
            "Validation accuray increased (48.333332 --> 50.000000).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0440, Accuracy: 26/60 (43%)\n",
            "\n",
            "\n",
            "현재 epoch : 4\n",
            "\n",
            "[  4/100] train_loss: 1.26550 valid_loss: 1.24094 valid_accuary: 50.0%\n",
            "Validation accuray increased (50.000000 --> 50.000000).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0402, Accuracy: 32/60 (53%)\n",
            "\n",
            "\n",
            "현재 epoch : 5\n",
            "\n",
            "[  5/100] train_loss: 1.26586 valid_loss: 1.20490 valid_accuary: 53.3%\n",
            "Validation accuray increased (50.000000 --> 53.333332).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0419, Accuracy: 29/60 (48%)\n",
            "\n",
            "\n",
            "현재 epoch : 6\n",
            "\n",
            "[  6/100] train_loss: 1.22721 valid_loss: 1.24224 valid_accuary: 50.0%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0409, Accuracy: 31/60 (52%)\n",
            "\n",
            "\n",
            "현재 epoch : 7\n",
            "\n",
            "[  7/100] train_loss: 1.22207 valid_loss: 1.25125 valid_accuary: 48.3%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0399, Accuracy: 32/60 (53%)\n",
            "\n",
            "\n",
            "현재 epoch : 8\n",
            "\n",
            "[  8/100] train_loss: 1.21451 valid_loss: 1.24764 valid_accuary: 48.3%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0396, Accuracy: 34/60 (57%)\n",
            "\n",
            "\n",
            "현재 epoch : 9\n",
            "\n",
            "[  9/100] train_loss: 1.21717 valid_loss: 1.20975 valid_accuary: 53.3%\n",
            "Validation accuray increased (53.333332 --> 53.333332).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0402, Accuracy: 32/60 (53%)\n",
            "\n",
            "\n",
            "현재 epoch : 10\n",
            "\n",
            "[ 10/100] train_loss: 1.19247 valid_loss: 1.25725 valid_accuary: 48.3%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0382, Accuracy: 36/60 (60%)\n",
            "\n",
            "\n",
            "현재 epoch : 11\n",
            "\n",
            "[ 11/100] train_loss: 1.19397 valid_loss: 1.21651 valid_accuary: 53.3%\n",
            "Validation accuray increased (53.333332 --> 53.333332).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0385, Accuracy: 35/60 (58%)\n",
            "\n",
            "\n",
            "현재 epoch : 12\n",
            "\n",
            "[ 12/100] train_loss: 1.18669 valid_loss: 1.23236 valid_accuary: 51.7%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0396, Accuracy: 33/60 (55%)\n",
            "\n",
            "\n",
            "현재 epoch : 13\n",
            "\n",
            "[ 13/100] train_loss: 1.19563 valid_loss: 1.22802 valid_accuary: 51.7%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0382, Accuracy: 36/60 (60%)\n",
            "\n",
            "\n",
            "현재 epoch : 14\n",
            "\n",
            "[ 14/100] train_loss: 1.23154 valid_loss: 1.22301 valid_accuary: 51.7%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0412, Accuracy: 31/60 (52%)\n",
            "\n",
            "\n",
            "현재 epoch : 15\n",
            "\n",
            "[ 15/100] train_loss: 1.21503 valid_loss: 1.16742 valid_accuary: 58.3%\n",
            "Validation accuray increased (53.333332 --> 58.333332).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 35/60 (58%)\n",
            "\n",
            "\n",
            "현재 epoch : 16\n",
            "\n",
            "[ 16/100] train_loss: 1.17764 valid_loss: 1.20790 valid_accuary: 53.3%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0392, Accuracy: 34/60 (57%)\n",
            "\n",
            "\n",
            "현재 epoch : 17\n",
            "\n",
            "[ 17/100] train_loss: 1.14636 valid_loss: 1.20473 valid_accuary: 53.3%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0401, Accuracy: 32/60 (53%)\n",
            "\n",
            "\n",
            "현재 epoch : 18\n",
            "\n",
            "[ 18/100] train_loss: 1.15345 valid_loss: 1.17906 valid_accuary: 56.7%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0404, Accuracy: 32/60 (53%)\n",
            "\n",
            "\n",
            "현재 epoch : 19\n",
            "\n",
            "[ 19/100] train_loss: 1.15593 valid_loss: 1.21552 valid_accuary: 53.3%\n",
            "EarlyStopping counter: 4 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0378, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 20\n",
            "\n",
            "[ 20/100] train_loss: 1.16348 valid_loss: 1.24137 valid_accuary: 50.0%\n",
            "EarlyStopping counter: 5 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0410, Accuracy: 31/60 (52%)\n",
            "\n",
            "\n",
            "현재 epoch : 21\n",
            "\n",
            "[ 21/100] train_loss: 1.12765 valid_loss: 1.23868 valid_accuary: 50.0%\n",
            "EarlyStopping counter: 6 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0378, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 22\n",
            "\n",
            "[ 22/100] train_loss: 1.13282 valid_loss: 1.14229 valid_accuary: 60.0%\n",
            "Validation accuray increased (58.333332 --> 60.000000).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 41/60 (68%)\n",
            "\n",
            "\n",
            "현재 epoch : 23\n",
            "\n",
            "[ 23/100] train_loss: 1.13326 valid_loss: 1.13082 valid_accuary: 61.7%\n",
            "Validation accuray increased (60.000000 --> 61.666668).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0373, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 24\n",
            "\n",
            "[ 24/100] train_loss: 1.11294 valid_loss: 1.08533 valid_accuary: 65.0%\n",
            "Validation accuray increased (61.666668 --> 65.000000).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0391, Accuracy: 35/60 (58%)\n",
            "\n",
            "\n",
            "현재 epoch : 25\n",
            "\n",
            "[ 25/100] train_loss: 1.12230 valid_loss: 1.06500 valid_accuary: 68.3%\n",
            "Validation accuray increased (65.000000 --> 68.333336).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0397, Accuracy: 33/60 (55%)\n",
            "\n",
            "\n",
            "현재 epoch : 26\n",
            "\n",
            "[ 26/100] train_loss: 1.09438 valid_loss: 1.06602 valid_accuary: 68.3%\n",
            "Validation accuray increased (68.333336 --> 68.333336).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0339, Accuracy: 44/60 (73%)\n",
            "\n",
            "\n",
            "현재 epoch : 27\n",
            "\n",
            "[ 27/100] train_loss: 1.10583 valid_loss: 1.17352 valid_accuary: 56.7%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 28\n",
            "\n",
            "[ 28/100] train_loss: 1.09580 valid_loss: 1.10770 valid_accuary: 63.3%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 34/60 (57%)\n",
            "\n",
            "\n",
            "현재 epoch : 29\n",
            "\n",
            "[ 29/100] train_loss: 1.07118 valid_loss: 1.06137 valid_accuary: 68.3%\n",
            "Validation accuray increased (68.333336 --> 68.333336).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0374, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 30\n",
            "\n",
            "[ 30/100] train_loss: 1.10377 valid_loss: 1.04494 valid_accuary: 68.3%\n",
            "Validation accuray increased (68.333336 --> 68.333336).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0339, Accuracy: 44/60 (73%)\n",
            "\n",
            "\n",
            "현재 epoch : 31\n",
            "\n",
            "[ 31/100] train_loss: 1.09433 valid_loss: 1.01265 valid_accuary: 73.3%\n",
            "Validation accuray increased (68.333336 --> 73.333336).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 43/60 (72%)\n",
            "\n",
            "\n",
            "현재 epoch : 32\n",
            "\n",
            "[ 32/100] train_loss: 1.06569 valid_loss: 1.06055 valid_accuary: 68.3%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 42/60 (70%)\n",
            "\n",
            "\n",
            "현재 epoch : 33\n",
            "\n",
            "[ 33/100] train_loss: 1.07557 valid_loss: 1.21242 valid_accuary: 53.3%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0399, Accuracy: 33/60 (55%)\n",
            "\n",
            "\n",
            "현재 epoch : 34\n",
            "\n",
            "[ 34/100] train_loss: 1.07093 valid_loss: 1.10284 valid_accuary: 63.3%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0381, Accuracy: 36/60 (60%)\n",
            "\n",
            "\n",
            "현재 epoch : 35\n",
            "\n",
            "[ 35/100] train_loss: 1.04398 valid_loss: 0.97461 valid_accuary: 76.7%\n",
            "Validation accuray increased (73.333336 --> 76.666664).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 42/60 (70%)\n",
            "\n",
            "\n",
            "현재 epoch : 36\n",
            "\n",
            "[ 36/100] train_loss: 1.09366 valid_loss: 1.10174 valid_accuary: 63.3%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0347, Accuracy: 42/60 (70%)\n",
            "\n",
            "\n",
            "현재 epoch : 37\n",
            "\n",
            "[ 37/100] train_loss: 1.11811 valid_loss: 1.04459 valid_accuary: 70.0%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0380, Accuracy: 36/60 (60%)\n",
            "\n",
            "\n",
            "현재 epoch : 38\n",
            "\n",
            "[ 38/100] train_loss: 1.04576 valid_loss: 1.05987 valid_accuary: 68.3%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 41/60 (68%)\n",
            "\n",
            "\n",
            "현재 epoch : 39\n",
            "\n",
            "[ 39/100] train_loss: 1.04626 valid_loss: 1.10100 valid_accuary: 65.0%\n",
            "EarlyStopping counter: 4 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0341, Accuracy: 43/60 (72%)\n",
            "\n",
            "\n",
            "현재 epoch : 40\n",
            "\n",
            "[ 40/100] train_loss: 1.05756 valid_loss: 1.10969 valid_accuary: 63.3%\n",
            "EarlyStopping counter: 5 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 41\n",
            "\n",
            "[ 41/100] train_loss: 1.02918 valid_loss: 1.05156 valid_accuary: 70.0%\n",
            "EarlyStopping counter: 6 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0325, Accuracy: 46/60 (77%)\n",
            "\n",
            "\n",
            "현재 epoch : 42\n",
            "\n",
            "[ 42/100] train_loss: 1.04324 valid_loss: 1.06557 valid_accuary: 66.7%\n",
            "EarlyStopping counter: 7 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 46/60 (77%)\n",
            "\n",
            "\n",
            "현재 epoch : 43\n",
            "\n",
            "[ 43/100] train_loss: 1.05106 valid_loss: 1.10362 valid_accuary: 63.3%\n",
            "EarlyStopping counter: 8 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 44\n",
            "\n",
            "[ 44/100] train_loss: 1.04694 valid_loss: 1.00579 valid_accuary: 75.0%\n",
            "EarlyStopping counter: 9 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0368, Accuracy: 38/60 (63%)\n",
            "\n",
            "\n",
            "현재 epoch : 45\n",
            "\n",
            "[ 45/100] train_loss: 1.02254 valid_loss: 0.97157 valid_accuary: 76.7%\n",
            "Validation accuray increased (76.666664 --> 76.666664).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0345, Accuracy: 43/60 (72%)\n",
            "\n",
            "\n",
            "현재 epoch : 46\n",
            "\n",
            "[ 46/100] train_loss: 1.03092 valid_loss: 0.97531 valid_accuary: 76.7%\n",
            "Validation accuray increased (76.666664 --> 76.666664).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0339, Accuracy: 44/60 (73%)\n",
            "\n",
            "\n",
            "현재 epoch : 47\n",
            "\n",
            "[ 47/100] train_loss: 1.03047 valid_loss: 0.99230 valid_accuary: 75.0%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 44/60 (73%)\n",
            "\n",
            "\n",
            "현재 epoch : 48\n",
            "\n",
            "[ 48/100] train_loss: 1.01958 valid_loss: 1.05005 valid_accuary: 68.3%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 41/60 (68%)\n",
            "\n",
            "\n",
            "현재 epoch : 49\n",
            "\n",
            "[ 49/100] train_loss: 1.01824 valid_loss: 1.03380 valid_accuary: 71.7%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 45/60 (75%)\n",
            "\n",
            "\n",
            "현재 epoch : 50\n",
            "\n",
            "[ 50/100] train_loss: 1.01551 valid_loss: 1.01376 valid_accuary: 73.3%\n",
            "EarlyStopping counter: 4 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0325, Accuracy: 46/60 (77%)\n",
            "\n",
            "\n",
            "현재 epoch : 51\n",
            "\n",
            "[ 51/100] train_loss: 1.01924 valid_loss: 1.05835 valid_accuary: 68.3%\n",
            "EarlyStopping counter: 5 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0379, Accuracy: 36/60 (60%)\n",
            "\n",
            "\n",
            "현재 epoch : 52\n",
            "\n",
            "[ 52/100] train_loss: 1.02197 valid_loss: 1.01574 valid_accuary: 73.3%\n",
            "EarlyStopping counter: 6 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0377, Accuracy: 37/60 (62%)\n",
            "\n",
            "\n",
            "현재 epoch : 53\n",
            "\n",
            "[ 53/100] train_loss: 1.00672 valid_loss: 1.03161 valid_accuary: 71.7%\n",
            "EarlyStopping counter: 7 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 44/60 (73%)\n",
            "\n",
            "\n",
            "현재 epoch : 54\n",
            "\n",
            "[ 54/100] train_loss: 1.01872 valid_loss: 0.99143 valid_accuary: 75.0%\n",
            "EarlyStopping counter: 8 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 45/60 (75%)\n",
            "\n",
            "\n",
            "현재 epoch : 55\n",
            "\n",
            "[ 55/100] train_loss: 1.05254 valid_loss: 0.98657 valid_accuary: 75.0%\n",
            "EarlyStopping counter: 9 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0327, Accuracy: 46/60 (77%)\n",
            "\n",
            "\n",
            "현재 epoch : 56\n",
            "\n",
            "[ 56/100] train_loss: 1.00178 valid_loss: 0.93843 valid_accuary: 80.0%\n",
            "Validation accuray increased (76.666664 --> 80.000000).  Saving model ...\n",
            "\n",
            "Test set: Average loss: 0.0366, Accuracy: 39/60 (65%)\n",
            "\n",
            "\n",
            "현재 epoch : 57\n",
            "\n",
            "[ 57/100] train_loss: 0.98052 valid_loss: 0.98051 valid_accuary: 76.7%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0340, Accuracy: 43/60 (72%)\n",
            "\n",
            "\n",
            "현재 epoch : 58\n",
            "\n",
            "[ 58/100] train_loss: 0.99317 valid_loss: 0.95597 valid_accuary: 78.3%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 41/60 (68%)\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9a5902727645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"전체 시도 횟수 {n}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtrain_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n {try_num}회 평균 정확도는 {sum(arr_acc)/try_num:.1f}%, 최댓값은 {max(arr_acc):.1f}%, 최솟값은 {min(arr_acc):.1f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1a1833193a13>\u001b[0m in \u001b[0;36mtrain_1\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-115cd83d98d5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "try_num = 20 # 전체 시도 회수\n",
        "arr_acc = []\n",
        "patience = 10  # earlystopping patience\n",
        "for n in range(1,try_num+1):\n",
        "    s = np.arange(image1.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image1_shuff = image1[s]       # 불러온 성진_motion data의 순서를 섞음\n",
        "\n",
        "    s = np.arange(image2.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image2_shuff = image2[s]       # 불러온 호정_motion data의 순서를 섞음\n",
        "\n",
        "    s = np.arange(image3.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image3_shuff = image3[s]       # 불러온 3_motion data의 순서를 섞음\n",
        "\n",
        "    s = np.arange(image4.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    image4_shuff = image4[s]       # 불러온 4_motion data의 순서를 섞음\n",
        "\n",
        "    image1_crop = preprocessing_resize_crop(\n",
        "        image1_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image2_crop = preprocessing_resize_crop(\n",
        "        image2_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image3_crop = preprocessing_resize_crop(\n",
        "        image3_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "    image4_crop = preprocessing_resize_crop(\n",
        "        image4_shuff, start_row, end_row, start_col, end_col, scale_row, scale_col)\n",
        "\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = concatenate_n_div(\n",
        "            image1_crop, label1, image2_crop, label2, image3_crop, label3, image4_crop, label4)\n",
        "                # 자른 image를 각 data set으로 나눠서 합침\n",
        "\n",
        "    #===========잠시 ==========\n",
        "    # maxval = x_train.max()\n",
        "    # if maxval < x_val.max():\n",
        "    #     maxval = x_val.max()\n",
        "    # if maxval < x_test.max():\n",
        "    #     maxval = x_test.max()\n",
        "    # # 정규화\n",
        "    # x_train = x_train.astype('float32')/maxval\n",
        "    # x_val = x_val.astype('float32')/maxval\n",
        "    # x_test = x_test.astype('float32')/maxval\n",
        "    x_train = (x_train - total_mean) / total_std\n",
        "    x_val = (x_val - total_mean) / total_std\n",
        "    x_test = (x_test - total_mean) / total_std\n",
        "\n",
        "    # 보강할 학습데이터 이미지 생성\n",
        "    if aug == 1:\n",
        "        augment_ratio = 9   # 전체 데이터의 150%\n",
        "        augment_size = int(augment_ratio * x_train.shape[0])\n",
        "\n",
        "        # 전체 x_train 개수의 150% 비율만큼\n",
        "        randidx = np.random.randint(x_train.shape[0], size=augment_size)\n",
        "\n",
        "        # 임의로 선택된 데이터는 원본데이터를 참조하기 때문에\n",
        "        # 원본데이터에 영향을 줄수 있음. 그래서 copy() 함수를 통해 안전하게 복사본 만듬\n",
        "        x_augmented = x_train[randidx].copy()  \n",
        "        y_augmented = y_train[randidx].copy()\n",
        "\n",
        "        #  이미지 보강 실행\n",
        "        x_augmented, y_augmented = gen.flow(x_augmented, y_augmented, \n",
        "                                            batch_size=augment_size,\n",
        "                                            shuffle=False).next()\n",
        "\n",
        "        x_train = np.concatenate((x_train,x_augmented))\n",
        "        y_train = np.concatenate((y_train,y_augmented))\n",
        "        s = np.arange(x_train.shape[0])\n",
        "        np.random.shuffle(s)\n",
        "        x_train = x_train[s]\n",
        "        y_train = y_train[s]\n",
        "\n",
        "    train_data = TensorData(x_train,y_train)\n",
        "    test_data = TensorData(x_test,y_test)\n",
        "    valid_data = TensorData(x_val,y_val)\n",
        "    train_loader = DataLoader(train_data, batch_size = 32, shuffle =True)\n",
        "    test_loader = DataLoader(test_data, batch_size = 32, shuffle =True)\n",
        "    valid_loader = DataLoader(valid_data, batch_size = 32, shuffle =True)\n",
        "\n",
        "    model = Net()\n",
        "    if args_cuda:\n",
        "        torch.cuda.set_device(0)\n",
        "        model.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    print(f\"전체 시도 횟수 {n}\")\n",
        "    train_1(100)\n",
        "\n",
        "print(f\"\\n {try_num}회 평균 정확도는 {sum(arr_acc)/try_num:.1f}%, 최댓값은 {max(arr_acc):.1f}%, 최솟값은 {min(arr_acc):.1f}%\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18nO_r0o6tzl",
        "outputId": "18338da8-2032-4711-d749-6775f8439b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 10회 평균 정확도는 72.3%, 최댓값은 85.0%, 최솟값은 60.0%\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n {try_num}회 평균 정확도는 {sum(arr_acc)/try_num:.1f}%, 최댓값은 {max(arr_acc):.1f}%, 최솟값은 {min(arr_acc):.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L4StYdx6tzm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "pytorch_loop.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6d6ff7ed2e65d333484b655795d870748eda876c6333e127a822707e48c97f40"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
